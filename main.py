import re
import json
from typing import Dict, List, Tuple, Optional, Any
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from dotenv import load_dotenv
import os
from pathlib import Path
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class JargonTranslator:
    """é»‘è¯ç¿»è¯‘å™¨ - ä»Graph RAGçŸ¥è¯†åº“ä¸­åŠ è½½æœ¯è¯­è¡¨"""
    
    def __init__(self, graph_db_path: str = None):
        self.graph_db_path = graph_db_path
        self.jargon_dict = {}
        if graph_db_path:
            self._load_jargon_from_graph_db()
    
    def _load_jargon_from_graph_db(self):
        """ä»Graph RAGçš„JSONæ–‡ä»¶åŠ è½½æœ¯è¯­è¡¨"""
        try:
            print("ä»Graph RAGæ•°æ®åº“åŠ è½½æœ¯è¯­è¡¨...")
            
            # åŠ è½½documents.json
            documents_path = Path(self.graph_db_path) / "documents.json"
            if not documents_path.exists():
                print(f"è­¦å‘Š: æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {documents_path}")
                return
            
            with open(documents_path, 'r', encoding='utf-8') as f:
                documents_data = json.load(f)
            
            print(f"æ‰¾åˆ° {len(documents_data)} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹æœç´¢æœ¯è¯­è¡¨...")
            
            # æŸ¥æ‰¾æœ¯è¯­è¡¨æ–‡æ¡£
            terminology_found = False
            for doc_id, doc_data in documents_data.items():
                content = doc_data.get("content", "")
                doc_type = doc_data.get("document_type", "")
                title = doc_data.get("title", "")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¯è¯­è¡¨
                if (doc_type == "Terminology Table" or 
                    "terminology" in title.lower() or
                    "æœ¯è¯­" in content or
                    self._looks_like_terminology(content)):
                    
                    print(f"æ‰¾åˆ°æœ¯è¯­è¡¨æ–‡æ¡£: {title}")
                    print(f"å†…å®¹é¢„è§ˆ: {content[:200]}...")
                    
                    # æå–æœ¯è¯­
                    extracted_jargon = self._extract_jargon_comprehensive(content)
                    
                    if extracted_jargon:
                        print(f"æˆåŠŸæå– {len(extracted_jargon)} ä¸ªæœ¯è¯­:")
                        for jargon, definition in extracted_jargon.items():
                            self.jargon_dict[jargon] = definition
                            print(f"  {jargon}: {definition}")
                        terminology_found = True
                        break
            
            if not terminology_found:
                print("è­¦å‘Š: æœªæ‰¾åˆ°æœ¯è¯­è¡¨ï¼Œä½¿ç”¨é»˜è®¤æœ¯è¯­")
                
            else:
                print(f"æœ¯è¯­è¡¨åŠ è½½å®Œæˆï¼Œå…± {len(self.jargon_dict)} ä¸ªæœ¯è¯­")
                        
        except Exception as e:
            print(f"æœ¯è¯­è¡¨åŠ è½½å¤±è´¥: {e}")
            
    
    def _looks_like_terminology(self, content: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦çœ‹èµ·æ¥åƒæœ¯è¯­è¡¨"""
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªå¤§å†™ç¼©å†™å’Œå†’å·å®šä¹‰çš„æ¨¡å¼
        abbreviation_pattern = r'\b[A-Z]{2,}\s*[:-]'
        matches = re.findall(abbreviation_pattern, content)
        return len(matches) >= 3  # è‡³å°‘æœ‰3ä¸ªç¼©å†™å®šä¹‰
    
    
    def _extract_jargon_comprehensive(self, content: str) -> Dict[str, str]:
        """ç»¼åˆæå–é»‘è¯æœ¯è¯­"""
        jargon_dict = {}
        
        # å¤šç§åŒ¹é…æ¨¡å¼
        patterns = [
            # æ ‡å‡†æ ¼å¼: ASL: Age-sensitive logic
            r'([A-Z]{2,}|[A-Z][a-z]+(?:[A-Z][a-z]*)*)\s*[:-]\s*([^\n\r]+)',
            # æ‹¬å·æ ¼å¼: ASL (Age-sensitive logic)
            r'([A-Z]{2,})\s*\(([^)]+)\)',
            # é‡Šä¹‰æ ¼å¼: ASL means Age-sensitive logic  
            r'([A-Z]{2,})\s+(?:means?|is|stands for|refers to)\s+([^\n\r\.]+)',
            # å¸¦å¼•å·: "ASL": Age-sensitive logic
            r'["\']([A-Z]{2,})["\']?\s*[:-]\s*([^\n\r]+)',
            # è¿å­—ç¬¦: ASL - Age-sensitive logic
            r'([A-Z]{2,})\s*[-â€“â€”]\s*([^\n\r]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
            for jargon, definition in matches:
                jargon = jargon.strip()
                definition = definition.strip().rstrip('.,;:|')
                
                # è´¨é‡è¿‡æ»¤
                if (len(jargon) >= 2 and len(definition) > 5 and 
                    len(definition) < 200 and
                    not definition.isupper() and
                    jargon.isupper()):  # ç¡®ä¿æœ¯è¯­æ˜¯å¤§å†™
                    jargon_dict[jargon] = definition
        
        return jargon_dict
    
    def translate_jargon(self, text: str) -> Tuple[str, List[str]]:
        """ç¿»è¯‘æ–‡æœ¬ä¸­çš„é»‘è¯"""
        if not self.jargon_dict:
            return text, []
            
        translated_text = text
        found_jargons = []
        
        # æŒ‰é•¿åº¦æ’åºï¼Œé¿å…éƒ¨åˆ†åŒ¹é…é—®é¢˜
        sorted_jargon = sorted(self.jargon_dict.items(), key=lambda x: len(x[0]), reverse=True)
        
        for jargon, definition in sorted_jargon:
            pattern = r'\b' + re.escape(jargon) + r'\b'
            if re.search(pattern, translated_text, re.IGNORECASE):
                found_jargons.append(jargon)
                replacement = f"{jargon} ({definition})"
                translated_text = re.sub(pattern, replacement, translated_text, flags=re.IGNORECASE)
        
        return translated_text, found_jargons


class OptimizedGraphRAGRetriever:
    """ä¼˜åŒ–çš„Graph RAGæ£€ç´¢å™¨"""
    
    def __init__(self, graph_db_path: str):
        self.graph_db_path = Path(graph_db_path)
        self.documents = {}
        self.jurisdictions = {}
        self.document_embeddings = {}
        self.embedding_model = None
        self._load_graph_data()
    
    def _load_graph_data(self):
        """åŠ è½½Graph RAGæ•°æ®"""
        try:
            print(f"åŠ è½½Graph RAGæ•°æ®ä»: {self.graph_db_path}")
            
            # åŠ è½½æ–‡æ¡£æ•°æ®
            documents_path = self.graph_db_path / "documents.json"
            if documents_path.exists():
                with open(documents_path, 'r', encoding='utf-8') as f:
                    documents_data = json.load(f)
                self.documents = documents_data
                print(f"âœ“ åŠ è½½äº† {len(documents_data)} ä¸ªæ–‡æ¡£")
            else:
                raise FileNotFoundError(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {documents_path}")
            
            # åŠ è½½ç®¡è¾–åŒºæ•°æ®
            jurisdictions_path = self.graph_db_path / "jurisdictions.json"  
            if jurisdictions_path.exists():
                with open(jurisdictions_path, 'r', encoding='utf-8') as f:
                    self.jurisdictions = json.load(f)
                print(f"âœ“ åŠ è½½äº† {len(self.jurisdictions)} ä¸ªç®¡è¾–åŒº")
            
            # åŠ è½½å‘é‡åµŒå…¥
            embeddings_path = self.graph_db_path / "embeddings.pkl"
            if embeddings_path.exists():
                with open(embeddings_path, 'rb') as f:
                    self.document_embeddings = pickle.load(f)
                print(f"âœ“ åŠ è½½äº† {len(self.document_embeddings)} ä¸ªæ–‡æ¡£çš„åµŒå…¥å‘é‡")
                
                # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                self.embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")
                print("âœ“ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°åµŒå…¥å‘é‡æ–‡ä»¶ï¼Œå°†ä½¿ç”¨å…³é”®è¯æœç´¢")
            
        except Exception as e:
            print(f"åŠ è½½Graph RAGæ•°æ®å¤±è´¥: {e}")
            raise e
    
    def similarity_search(self, query: str, k: int = 5, jurisdictions: List[str] = None) -> List[Dict]:
        """ä¼˜åŒ–çš„è¯­ä¹‰ç›¸ä¼¼æ€§æœç´¢"""
        if not self.embedding_model or not self.document_embeddings:
            print("ä½¿ç”¨å…³é”®è¯æœç´¢æ¨¡å¼")
            return self._keyword_search(query, k, jurisdictions)
        
        try:
            print(f"æ‰§è¡Œå‘é‡æœç´¢: {query}")
            
            # æŸ¥è¯¢å‘é‡åŒ–
            query_embedding = self.embedding_model.embed_query(query)
            query_embedding = np.array(query_embedding).reshape(1, -1)
            
            results = []
            
            # ç­›é€‰ç›®æ ‡æ–‡æ¡£
            if jurisdictions:
                target_doc_ids = self._filter_documents_by_jurisdiction(jurisdictions)
                print(f"åŸºäºç®¡è¾–åŒº {jurisdictions} ç­›é€‰åˆ° {len(target_doc_ids)} ä¸ªæ–‡æ¡£")
            else:
                target_doc_ids = list(self.documents.keys())
                print(f"æœç´¢å…¨éƒ¨ {len(target_doc_ids)} ä¸ªæ–‡æ¡£")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            for doc_id in target_doc_ids:
                if doc_id not in self.document_embeddings:
                    continue
                
                doc_data = self.documents[doc_id]
                doc_embeddings = self.document_embeddings[doc_id]
                chunks = doc_data.get('chunks', [])
                
                if len(chunks) == 0 or doc_embeddings.shape[0] == 0:
                    continue
                
                # æ‰¹é‡è®¡ç®—ç›¸ä¼¼åº¦
                similarities = cosine_similarity(query_embedding, doc_embeddings)[0]
                
                # æ”¶é›†é«˜è´¨é‡ç»“æœ
                for i, (chunk, similarity) in enumerate(zip(chunks, similarities)):
                    if similarity > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                        results.append({
                            'page_content': chunk,
                            'similarity_score': float(similarity),
                            'metadata': {
                                'document_id': doc_id,
                                'document_title': doc_data.get('title', 'Unknown'),
                                'document_type': doc_data.get('document_type', 'Unknown'),
                                'jurisdiction_id': doc_data.get('jurisdiction_id', 'Unknown'),
                                'chunk_index': i,
                                'source': doc_data.get('metadata', {}).get('source_path', 'Graph RAG DB'),
                                'classification_confidence': doc_data.get('metadata', {}).get('classification_confidence', 0.5)
                            }
                        })
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x['similarity_score'], reverse=True)
            final_results = results[:k]
            
            print(f"æ‰¾åˆ° {len(final_results)} ä¸ªé«˜è´¨é‡ç»“æœ")
            return final_results
            
        except Exception as e:
            print(f"å‘é‡æœç´¢å¤±è´¥ï¼Œåˆ‡æ¢åˆ°å…³é”®è¯æœç´¢: {e}")
            return self._keyword_search(query, k, jurisdictions)
    
    def _filter_documents_by_jurisdiction(self, jurisdictions: List[str]) -> List[str]:
        """æ ¹æ®ç®¡è¾–åŒºç­›é€‰æ–‡æ¡£ï¼ˆåŒ…å«å±‚çº§å…³ç³»ï¼‰"""
        filtered_doc_ids = set()
        
        for jurisdiction in jurisdictions:
            # ç›´æ¥åŒ…å«è¯¥ç®¡è¾–åŒºçš„æ–‡æ¡£
            if jurisdiction in self.jurisdictions:
                jur_data = self.jurisdictions[jurisdiction]
                doc_ids = jur_data.get('document_ids', [])
                filtered_doc_ids.update(doc_ids)
                
                # åŒ…å«çˆ¶ç®¡è¾–åŒºçš„æ–‡æ¡£ï¼ˆæ³•å¾‹å±‚çº§ç»§æ‰¿ï¼‰
                parent_id = jur_data.get('parent_id')
                if parent_id and parent_id in self.jurisdictions:
                    parent_doc_ids = self.jurisdictions[parent_id].get('document_ids', [])
                    filtered_doc_ids.update(parent_doc_ids)
        
        return list(filtered_doc_ids)
    
    def _keyword_search(self, query: str, k: int = 5, jurisdictions: List[str] = None) -> List[Dict]:
        """ä¼˜åŒ–çš„å…³é”®è¯æœç´¢"""
        print(f"æ‰§è¡Œå…³é”®è¯æœç´¢: {query}")
        
        # é¢„å¤„ç†æŸ¥è¯¢è¯
        query_words = [word.lower() for word in query.split() if len(word) > 2]
        results = []
        
        # ç­›é€‰æ–‡æ¡£
        if jurisdictions:
            target_doc_ids = self._filter_documents_by_jurisdiction(jurisdictions)
        else:
            target_doc_ids = list(self.documents.keys())
        
        print(f"åœ¨ {len(target_doc_ids)} ä¸ªæ–‡æ¡£ä¸­æœç´¢")
        
        for doc_id in target_doc_ids:
            doc_data = self.documents[doc_id]
            chunks = doc_data.get('chunks', [])
            
            # åœ¨chunksä¸­æœç´¢
            if chunks:
                for i, chunk in enumerate(chunks):
                    chunk_lower = chunk.lower()
                    
                    # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
                    exact_matches = sum(1 for word in query_words if word in chunk_lower)
                    partial_matches = sum(0.5 for word in query_words 
                                        if any(word in token for token in chunk_lower.split()))
                    
                    total_score = exact_matches + partial_matches
                    
                    if total_score > 0:
                        results.append({
                            'page_content': chunk,
                            'keyword_score': total_score,
                            'metadata': {
                                'document_id': doc_id,
                                'document_title': doc_data.get('title', 'Unknown'),
                                'document_type': doc_data.get('document_type', 'Unknown'), 
                                'jurisdiction_id': doc_data.get('jurisdiction_id', 'Unknown'),
                                'chunk_index': i,
                                'source': doc_data.get('metadata', {}).get('source_path', 'Graph RAG DB'),
                                'classification_confidence': doc_data.get('metadata', {}).get('classification_confidence', 0.5)
                            }
                        })
            else:
                # åœ¨æ•´ä¸ªæ–‡æ¡£ä¸­æœç´¢
                content = doc_data.get('content', '').lower()
                exact_matches = sum(1 for word in query_words if word in content)
                
                if exact_matches > 0:
                    # æå–åŒ…å«å…³é”®è¯çš„ç‰‡æ®µ
                    content_excerpt = self._extract_relevant_excerpt(
                        doc_data.get('content', ''), query_words[0] if query_words else query
                    )
                    
                    results.append({
                        'page_content': content_excerpt,
                        'keyword_score': exact_matches,
                        'metadata': {
                            'document_id': doc_id,
                            'document_title': doc_data.get('title', 'Unknown'),
                            'document_type': doc_data.get('document_type', 'Unknown'),
                            'jurisdiction_id': doc_data.get('jurisdiction_id', 'Unknown'),
                            'source': doc_data.get('metadata', {}).get('source_path', 'Graph RAG DB'),
                            'classification_confidence': doc_data.get('metadata', {}).get('classification_confidence', 0.5)
                        }
                    })
        
        # æ’åºå¹¶è¿”å›
        score_key = 'keyword_score' if results and 'keyword_score' in results[0] else 'similarity_score'
        results.sort(key=lambda x: x.get(score_key, 0), reverse=True)
        
        final_results = results[:k]
        print(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(final_results)} ä¸ªç»“æœ")
        return final_results
    
    def _extract_relevant_excerpt(self, content: str, keyword: str, max_length: int = 800) -> str:
        """æå–åŒ…å«å…³é”®è¯çš„ç›¸å…³ç‰‡æ®µ"""
        keyword_lower = keyword.lower()
        content_lower = content.lower()
        
        # æ‰¾åˆ°å…³é”®è¯ä½ç½®
        pos = content_lower.find(keyword_lower)
        if pos == -1:
            return content[:max_length]
        
        # æå–å…³é”®è¯å‘¨å›´çš„å†…å®¹
        start = max(0, pos - max_length // 2)
        end = min(len(content), pos + max_length // 2)
        
        excerpt = content[start:end]
        
        # æ¸…ç†æˆªæ–­çš„å¥å­
        if start > 0:
            first_period = excerpt.find('.')
            if first_period > 0:
                excerpt = excerpt[first_period + 1:]
        
        if end < len(content):
            last_period = excerpt.rfind('.')
            if last_period > 0:
                excerpt = excerpt[:last_period + 1]
        
        return excerpt.strip()


class GeographicJurisdictionDetector:
    """åœ°ç†ç®¡è¾–åŒºæ£€æµ‹å™¨"""
    
    def __init__(self):
        self.location_patterns = self._build_location_patterns()
    
    def _build_location_patterns(self) -> Dict[str, List[str]]:
        """æ„å»ºåœ°ç†ä½ç½®åŒ¹é…æ¨¡å¼"""
        patterns = {
            # ç¾å›½å„å·
            "california": ["california", "ca ", "calif", "golden state"],
            "utah": ["utah", "ut ", "beehive state"],
            "florida": ["florida", "fl ", "fla", "sunshine state"],
            "texas": ["texas", "tx ", "tex", "lone star"],
            "usa": ["usa", "united states", "us ", "america", "federal", "u.s."],
            
            # æ¬§ç›ŸåŠæˆå‘˜å›½
            "eu": ["eu ", "european union", "europe", "eea", "european economic area"],
            "germany": ["germany", "german", "deutschland", "de "],
            "france": ["france", "french", "fr "],
            "italy": ["italy", "italian", "it "],
            "spain": ["spain", "spanish", "es "],
            "netherlands": ["netherlands", "dutch", "holland", "nl "],
            
            # å‚è€ƒæ–‡æ¡£
            "reference": ["terminology", "definitions", "glossary", "reference"]
        }
        return patterns
    
    def detect_jurisdictions(self, text: str) -> List[str]:
        """æ£€æµ‹æ–‡æœ¬ä¸­æ¶‰åŠçš„ç®¡è¾–åŒº"""
        text_lower = text.lower()
        detected_jurisdictions = []
        
        for jurisdiction_id, patterns in self.location_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    if jurisdiction_id not in detected_jurisdictions:
                        detected_jurisdictions.append(jurisdiction_id)
                    break
        
        # å¤„ç†å±‚çº§å…³ç³»
        state_to_country = {
            "california": "usa", "utah": "usa", "florida": "usa", "texas": "usa",
            "germany": "eu", "france": "eu", "italy": "eu", "spain": "eu", "netherlands": "eu"
        }
        
        for jurisdiction in detected_jurisdictions.copy():
            if jurisdiction in state_to_country:
                parent = state_to_country[jurisdiction]
                if parent not in detected_jurisdictions:
                    detected_jurisdictions.append(parent)
        
        return detected_jurisdictions


class OptimizedLegalClassifier:
    """ä¼˜åŒ–çš„æ³•å¾‹åˆ†ç±»å™¨ - çº¯Graph RAGç‰ˆæœ¬"""
    
    def __init__(self, 
                 graph_db_path: str = "./legal_graph_db",
                 model_name: str = "qwen-max"):
        
        print("=== åˆå§‹åŒ–ä¼˜åŒ–æ³•å¾‹åˆ†ç±»å™¨ (çº¯Graph RAGç‰ˆæœ¬) ===")
        
        # æ£€æŸ¥æ•°æ®åº“è·¯å¾„
        if not Path(graph_db_path).exists():
            raise FileNotFoundError(f"Graph RAGæ•°æ®åº“ä¸å­˜åœ¨: {graph_db_path}")
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        print("1. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹...")
        self.llm = ChatTongyi(model=model_name, temperature=0.1)
        
        print("2. åŠ è½½Graph RAGæ£€ç´¢å™¨...")
        self.retriever = OptimizedGraphRAGRetriever(graph_db_path)
        
        print("3. åˆå§‹åŒ–é»‘è¯ç¿»è¯‘å™¨...")
        self.jargon_translator = JargonTranslator(graph_db_path)
        
        print("4. åˆå§‹åŒ–åœ°ç†ç®¡è¾–åŒºæ£€æµ‹å™¨...")
        self.geo_detector = GeographicJurisdictionDetector()
        
        print("5. åˆå§‹åŒ–æç¤ºæ¨¡æ¿...")
        self.parser = JsonOutputParser()
        self.prompt = self._create_enhanced_prompt()
        
        print("âœ“ åˆå§‹åŒ–å®Œæˆï¼\n")
    
    def _create_enhanced_prompt(self) -> PromptTemplate:
        """åˆ›å»ºå¢å¼ºçš„æç¤ºæ¨¡æ¿"""
        template = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ³•å¾‹åˆè§„åˆ†æå¸ˆï¼Œå…·æœ‰Graph RAGæ³•å¾‹çŸ¥è¯†å›¾è°±æœç´¢èƒ½åŠ›ã€‚

ä»»åŠ¡ï¼šå°†åŠŸèƒ½ç‰¹æ€§å‡†ç¡®åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç±»ä¹‹ä¸€ï¼š
- "LegalRequirement"      (æ³•å¾‹/æ³•è§„/ç›‘ç®¡è¦æ±‚å¼ºåˆ¶æ‰§è¡Œ)
- "BusinessDriven"        (äº§å“ç­–ç•¥/å®éªŒ/å®‰å…¨é€‰æ‹©ï¼›éæ³•å¾‹å¼ºåˆ¶è¦æ±‚)
- "UnspecifiedNeedsHuman" (æ„å›¾ä¸æ˜ç¡®æˆ–è¯æ®ç¼ºå¤±/å†²çª)

åˆ†æè¾“å…¥ï¼š
åŸå§‹åŠŸèƒ½: {original_feature}

ç¿»è¯‘ååŠŸèƒ½(å«é»‘è¯è§£é‡Š): {translated_feature}

æ£€æµ‹åˆ°çš„ç®¡è¾–åŒº: {detected_jurisdictions}

å‘ç°çš„é»‘è¯æœ¯è¯­: {found_jargons}

Graph RAGæœç´¢çš„æ³•å¾‹è¯æ®:
{context}

å†³ç­–æ¡†æ¶ï¼š
LegalRequirement (å¾—åˆ† 0-1)ï¼š
+0.40 ä¸Šä¸‹æ–‡å¼•ç”¨ç‰¹å®šæ³•å¾‹æ¡æ–‡ + ç®¡è¾–åŒºåŒ¹é…
+0.20 åŠŸèƒ½è¡Œä¸ºæ˜ç¡®å¯¹åº”æ³•å¾‹è¦æ±‚(å¦‚å¹´é¾„é™åˆ¶â†”å„¿ç«¥ä¿æŠ¤æ³•)
+0.20 åœ°ç†é™åˆ¶ç¬¦åˆæ³•å¾‹ç®¡è¾–è¾¹ç•Œ
+0.20 è‡³å°‘æœ‰ä¸€ä¸ªå¯ä¿¡çš„æ³•å¾‹æ¡æ–‡å¼•ç”¨

BusinessDriven (å¾—åˆ† 0-1)ï¼š
+0.50 æ˜ç¡®å•†ä¸šåŠ¨æœºï¼šA/Bæµ‹è¯•ã€å®éªŒã€æ€§èƒ½ä¼˜åŒ–ã€å¢é•¿
+0.30 æ£€æµ‹åˆ°ç®¡è¾–åŒºä½†ç¼ºä¹æ³•å¾‹è¯æ®æ”¯æŒ
+0.20 åœ°ç†å·®å¼‚ä¸»è¦ç”¨äºäº§å“æ¨å‡ºç­–ç•¥

UnspecifiedNeedsHuman (å¾—åˆ† 0-1)ï¼š
+0.50 ç®¡è¾–åŒºæ£€æµ‹ä½†æ³•å¾‹è¯æ®ä¸è¶³æˆ–å†²çª
+0.30 åŠŸèƒ½æ„å›¾ä¸æ˜ç¡®ï¼Œå­˜åœ¨æ··åˆä¿¡å·
+0.20 åœ°åŒºé™åˆ¶æ— æ˜ç¡®æ³•å¾‹æˆ–å•†ä¸šç†ç”±

è¾“å‡ºè¦æ±‚ï¼š
{{
  "assessment": "LegalRequirement" | "BusinessDriven" | "UnspecifiedNeedsHuman",
  "needs_compliance_logic": true | false | null,
  "reasoning": "åŸºäºè¯æ®çš„è¯¦ç»†åˆ†æ â‰¤200å­—",
  "detected_jurisdictions": {detected_jurisdictions},
  "translated_jargon": {found_jargons},
  "jurisdictions": ["ç¡®å®šçš„é€‚ç”¨ç®¡è¾–åŒº"],
  "regulations": [
    {{
      "id": "æ³•è§„IDæˆ–null",
      "title": "æ³•è§„åç§°", 
      "jurisdiction": "ç®¡è¾–åŒº",
      "relevance": 0.0-1.0,
      "passages": [
        {{"quote": "â‰¤200å­—ç¬¦çš„å…³é”®æ¡æ–‡", "source_id": "æ–‡æ¡£ID"}}
      ],
      "decision": "Constrained" | "NotConstrained" | "Unclear",
      "reason": "è¯¥æ³•è§„å¦‚ä½•çº¦æŸæ­¤åŠŸèƒ½"
    }}
  ],
  "triggers": {{
    "legal": ["è¯†åˆ«çš„æ³•å¾‹å…³é”®è¯"],
    "business": ["è¯†åˆ«çš„å•†ä¸šå…³é”®è¯"],
    "ambiguity": ["æ¨¡ç³Šæˆ–å†²çªçš„è¡¨è¿°"]
  }},
  "scores": {{
    "LegalRequirement": 0.0-1.0,
    "BusinessDriven": 0.0-1.0,
    "UnspecifiedNeedsHuman": 0.0-1.0
  }},
  "citations": ["å¼•ç”¨çš„æ¥æºæ–‡æ¡£ID"],
  "confidence": 0.10-0.99
}}

çº¦æŸï¼š
- ä»…ä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„å®é™…æ³•å¾‹è¯æ®
- assessment â‰  "LegalRequirement" æ—¶ï¼Œregulations=[]
- ä¸å¾—ç¼–é€ æ³•å¾‹å¼•ç”¨
- ç®¡è¾–åŒºä»£ç å¿…é¡»ä¸æ•°æ®åº“åŒ¹é…
"""
        
        return PromptTemplate(
            template=template,
            input_variables=[
                "original_feature", "translated_feature", "detected_jurisdictions", 
                "found_jargons", "context"
            ]
        )
    
    def _search_legal_evidence(self, query: str, jurisdictions: List[str] = None) -> List[str]:
        """æœç´¢æ³•å¾‹è¯æ®"""
        try:
            print(f"å¼€å§‹Graph RAGæ³•å¾‹è¯æ®æœç´¢...")
            
            # æ„å»ºå¤šæ ·åŒ–çš„æœç´¢æŸ¥è¯¢
            search_queries = [query]
            
            # åŸºäºç®¡è¾–åŒºçš„ç‰¹å®šæŸ¥è¯¢
            if jurisdictions:
                jurisdiction_specific = {
                    "utah": ["Utah Social Media Act", "minor curfew", "parental consent"],
                    "california": ["California SB976", "addictive feed", "teen protection"],
                    "florida": ["Florida minor protection", "harmful content"],
                    "eu": ["Digital Services Act", "DSA", "content moderation"],
                    "usa": ["NCMEC", "COPPA", "child protection", "federal reporting"]
                }
                
                for jurisdiction in jurisdictions:
                    if jurisdiction in jurisdiction_specific:
                        for term in jurisdiction_specific[jurisdiction]:
                            search_queries.append(f"{term}")
                            search_queries.append(f"{term} {query}")
            
            # ä¸»é¢˜ç›¸å…³æŸ¥è¯¢
            query_lower = query.lower()
            if any(term in query_lower for term in ["minor", "child", "underage", "youth"]):
                search_queries.extend(["child protection law", "minor safety", "parental consent"])
            
            if "feed" in query_lower or "personalization" in query_lower:
                search_queries.extend(["algorithmic feed", "personalized content", "addictive design"])
                
            if "curfew" in query_lower:
                search_queries.extend(["time restrictions", "access limitations"])
            
            # æ‰§è¡Œæœç´¢
            all_results = []
            seen_docs = set()
            
            print(f"æ‰§è¡Œ {len(search_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
            
            for i, search_query in enumerate(search_queries[:8]):  # é™åˆ¶æŸ¥è¯¢æ•°é‡
                try:
                    print(f"  æŸ¥è¯¢ {i+1}: {search_query}")
                    docs = self.retriever.similarity_search(
                        search_query, k=3, jurisdictions=jurisdictions
                    )
                    
                    for doc in docs:
                        doc_id = doc['metadata'].get('document_id')
                        chunk_id = f"{doc_id}_{doc['metadata'].get('chunk_index', 0)}"
                        
                        # é¿å…é‡å¤
                        if chunk_id in seen_docs:
                            continue
                        seen_docs.add(chunk_id)
                        
                        # æ„å»ºç»“æœä¿¡æ¯
                        score_info = ""
                        if 'similarity_score' in doc:
                            score_info = f"Similarity: {doc['similarity_score']:.3f}"
                        elif 'keyword_score' in doc:
                            score_info = f"KeywordScore: {doc['keyword_score']}"
                        
                        source_info = f"[GraphRAG-{len(all_results)+1}] "
                        source_info += f"Query: {search_query[:30]}... | "
                        source_info += f"{score_info} | "
                        source_info += f"Doc: {doc['metadata'].get('document_title', 'Unknown')} | "
                        source_info += f"Type: {doc['metadata'].get('document_type', 'Unknown')} | "
                        source_info += f"Jurisdiction: {doc['metadata'].get('jurisdiction_id', 'Unknown')}"
                        
                        content = f"{source_info}\nContent: {doc['page_content'][:1000]}"
                        all_results.append(content)
                        
                        print(f"    âœ“ æ·»åŠ ç»“æœ: {doc['metadata'].get('document_title', 'Unknown')[:30]}")
                        
                        if len(all_results) >= 8:
                            break
                    
                except Exception as e:
                    print(f"    æŸ¥è¯¢å¤±è´¥: {e}")
                
                if len(all_results) >= 8:
                    break
            
            print(f"Graph RAGæœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_results)} æ¡æ³•å¾‹è¯æ®")
            
            # å¦‚æœç»“æœä¸è¶³ï¼Œè¿›è¡Œè¡¥å……æœç´¢
            if len(all_results) < 3:
                print("ç»“æœä¸è¶³ï¼Œæ‰§è¡Œè¡¥å……æœç´¢...")
                fallback_queries = ["law", "regulation", "requirement", "compliance"]
                for fallback_query in fallback_queries:
                    try:
                        docs = self.retriever.similarity_search(fallback_query, k=2)
                        for doc in docs[:1]:  # åªå–ä¸€ä¸ª
                            content = f"[GraphRAG-Fallback] {fallback_query}\n{doc['page_content'][:800]}"
                            all_results.append(content)
                            print(f"  æ·»åŠ è¡¥å……ç»“æœ: {doc['metadata'].get('document_title', 'Unknown')}")
                            if len(all_results) >= 5:
                                break
                    except:
                        continue
                    if len(all_results) >= 5:
                        break
            
            return all_results
            
        except Exception as e:
            print(f"Graph RAGæœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [f"æœç´¢å¤±è´¥: {str(e)}"]
    
    def classify_feature(self, feature_description: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„åŠŸèƒ½ç‰¹æ€§åˆ†ç±»æµç¨‹
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹åˆ†æåŠŸèƒ½: {feature_description[:80]}...")
        print(f"{'='*60}")
        
        # æ­¥éª¤1: é»‘è¯è¯†åˆ«ä¸ç¿»è¯‘
        print("æ­¥éª¤1: é»‘è¯è¯†åˆ«ä¸ç¿»è¯‘")
        translated_text, found_jargons = self.jargon_translator.translate_jargon(feature_description)
        
        if found_jargons:
            print(f"  âœ“ å‘ç°é»‘è¯: {found_jargons}")
            print(f"  âœ“ ç¿»è¯‘ç»“æœ: {translated_text}")
        else:
            print("  - æœªå‘ç°é»‘è¯æœ¯è¯­")
        
        # æ­¥éª¤2: åœ°ç†ç®¡è¾–åŒºæ£€æµ‹
        print("æ­¥éª¤2: åœ°ç†ç®¡è¾–åŒºæ£€æµ‹")
        detected_jurisdictions = self.geo_detector.detect_jurisdictions(feature_description)
        if detected_jurisdictions:
            print(f"  âœ“ æ£€æµ‹åˆ°ç®¡è¾–åŒº: {detected_jurisdictions}")
        else:
            print("  - æœªæ£€æµ‹åˆ°ç‰¹å®šç®¡è¾–åŒº")
        
        # æ­¥éª¤3: æ³•å¾‹è¯æ®æœç´¢
        print("æ­¥éª¤3: Graph RAGæ³•å¾‹è¯æ®æœç´¢")
        search_query = f"{feature_description} legal requirements compliance regulation"
        
        legal_evidence = self._search_legal_evidence(search_query, detected_jurisdictions)
        print(f"  âœ“ æ‰¾åˆ° {len(legal_evidence)} æ¡æ³•å¾‹è¯æ®")
        
        # æ­¥éª¤4: æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join(legal_evidence) if legal_evidence else "NO_LEGAL_EVIDENCE_FOUND"
        
        # æ­¥éª¤5: LLMåˆ†æä¸åˆ†ç±»
        print("æ­¥éª¤5: å¤§æ¨¡å‹åˆ†æä¸åˆ†ç±»")
        
        try:
            inputs = {
                "original_feature": feature_description,
                "translated_feature": translated_text,
                "detected_jurisdictions": detected_jurisdictions,
                "found_jargons": found_jargons,
                "context": context[:5000]  # æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            }
            
            formatted_prompt = self.prompt.format(**inputs)
            response = self.llm.invoke(formatted_prompt)
            result = self.parser.parse(response.content)
            
            # æ·»åŠ å¤„ç†å…ƒæ•°æ®
            result["processing_metadata"] = {
                "workflow_completed": True,
                "data_source": "Pure Graph RAG",
                "jargons_found": found_jargons,
                "jargons_translated": bool(found_jargons),
                "jurisdictions_detected": detected_jurisdictions,
                "legal_evidence_count": len(legal_evidence),
                "context_length": len(context),
                "embedding_search_available": bool(self.retriever.embedding_model),
                "search_method": "Vector" if self.retriever.embedding_model else "Keyword"
            }
            
            print(f"  âœ“ åˆ†ç±»å®Œæˆ: {result.get('assessment', 'Unknown')}")
            print(f"  âœ“ ç½®ä¿¡åº¦: {result.get('confidence', 'Unknown')}")
            if result.get('needs_compliance_logic') is not None:
                print(f"  âœ“ éœ€è¦åˆè§„é€»è¾‘: {result.get('needs_compliance_logic')}")
            
            return result
            
        except Exception as e:
            print(f"  âœ— åˆ†ç±»å¤±è´¥: {e}")
            return {
                "assessment": "UnspecifiedNeedsHuman",
                "needs_compliance_logic": None,
                "reasoning": f"åˆ†ç±»è¿‡ç¨‹å‡ºé”™: {str(e)}",
                "detected_jurisdictions": detected_jurisdictions,
                "translated_jargon": found_jargons,
                "error": str(e),
                "processing_metadata": {
                    "workflow_completed": False,
                    "data_source": "Pure Graph RAG",
                    "error": str(e),
                    "jargons_found": found_jargons,
                    "jurisdictions_detected": detected_jurisdictions
                }
            }
    
    def batch_classify(self, feature_list: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ç±»åŠŸèƒ½"""
        results = []
        total = len(feature_list)
        
        print(f"å¼€å§‹æ‰¹é‡åˆ†ç±» {total} ä¸ªåŠŸèƒ½ç‰¹æ€§")
        
        for i, feature in enumerate(feature_list, 1):
            print(f"\næ‰¹é‡è¿›åº¦: {i}/{total}")
            try:
                result = self.classify_feature(feature)
                results.append({
                    'index': i,
                    'input': feature,
                    'output': result,
                    'success': True
                })
            except Exception as e:
                print(f"åŠŸèƒ½ {i} åˆ†ç±»å¤±è´¥: {e}")
                results.append({
                    'index': i,
                    'input': feature,
                    'output': {'error': str(e)},
                    'success': False
                })
        
        success_count = sum(1 for r in results if r['success'])
        print(f"\næ‰¹é‡åˆ†ç±»å®Œæˆ: {success_count}/{total} æˆåŠŸ")
        
        return results


def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    print("=== ä¼˜åŒ–æ³•å¾‹åˆ†ç±»ç³»ç»Ÿ - çº¯Graph RAGç‰ˆæœ¬ ===\n")
    
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    try:
        # ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
        graph_db_path = "./legal_graph_db"
        
        classifier = OptimizedLegalClassifier(graph_db_path=graph_db_path)
        print("âœ“ åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ\n")
        
    except Exception as e:
        print(f"âœ— åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. Graph RAGæ•°æ®åº“è·¯å¾„æ˜¯å¦æ­£ç¡®")
        print("2. æ˜¯å¦å·²è¿è¡Œç¬¬ä¸€ä»½ä»£ç ç”Ÿæˆæ•°æ®åº“")
        print("3. æ•°æ®åº“æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶: documents.json, jurisdictions.json, embeddings.pkl")
        return
    
    # æµ‹è¯•æ•°æ®
    test_features = [
        {
            'name': 'Curfew login blocker with ASL and GH for Utah minors',
            'description': 'To comply with the Utah Social Media Regulation Act, we are implementing a curfew-based login restriction for users under 18. The system uses ASL to detect minor accounts and routes enforcement through GH to apply only within Utah boundaries. The feature activates during restricted night hours and logs activity using EchoTrace for auditability. This allows parental control to be enacted without user-facing alerts, operating in ShadowMode during initial rollout.'
        }
    ]
    
    # æ‰§è¡Œåˆ†ç±»æµ‹è¯•
    results = []
    for i, test_case in enumerate(test_features):
        print(f"\n{'='*80}")
        print(f"æµ‹è¯•æ¡ˆä¾‹ {i+1}/{len(test_features)}: {test_case['name']}")
        print(f"{'='*80}")
        
        # æ‰§è¡Œåˆ†ç±»
        result = classifier.classify_feature(test_case['description'])
        
        # ä¿å­˜ç»“æœ
        test_result = {
            'case_name': test_case['name'],
            'input': test_case['description'],
            'output': result
        }
        results.append(test_result)
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        print(f"\nğŸ“‹ åˆ†ç±»ç»“æœæ‘˜è¦:")
        print(f"  ğŸ·ï¸  åˆ†ç±»: {result.get('assessment', 'Unknown')}")
        print(f"  ğŸ”§ éœ€è¦åˆè§„é€»è¾‘: {result.get('needs_compliance_logic', 'Unknown')}")
        print(f"  ğŸ“Š ç½®ä¿¡åº¦: {result.get('confidence', 'Unknown')}")
        
        if result.get('detected_jurisdictions'):
            print(f"  ğŸŒ æ£€æµ‹ç®¡è¾–åŒº: {result.get('detected_jurisdictions')}")
        
        if result.get('translated_jargon'):
            print(f"  ğŸ”¤ å‘ç°é»‘è¯: {result.get('translated_jargon')}")
        
        reasoning = result.get('reasoning', '')
        if reasoning:
            print(f"  ğŸ’­ æ¨ç†: {reasoning[:100]}{'...' if len(reasoning) > 100 else ''}")
        
        # æ˜¾ç¤ºå¤„ç†å…ƒæ•°æ®
        if result.get('processing_metadata'):
            metadata = result['processing_metadata']
            print(f"\nğŸ” å¤„ç†ç»Ÿè®¡:")
            print(f"  âœ… å·¥ä½œæµå®Œæˆ: {metadata.get('workflow_completed', False)}")
            print(f"  ğŸ” æœç´¢æ–¹æ³•: {metadata.get('search_method', 'Unknown')}")
            print(f"  ğŸ“„ æ³•å¾‹è¯æ®æ•°: {metadata.get('legal_evidence_count', 0)}")
            print(f"  ğŸ“ ä¸Šä¸‹æ–‡é•¿åº¦: {metadata.get('context_length', 0)}")
        
        print(f"\n{'-'*60}")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    try:
        output_file = 'optimized_graph_rag_classification_results.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print(f"ğŸ“Š æ€»è®¡å¤„ç†: {len(results)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        
        # ç»Ÿè®¡åˆ†ç±»ç»“æœ
        assessments = [r['output'].get('assessment', 'Error') for r in results]
        from collections import Counter
        assessment_counts = Counter(assessments)
        
        print(f"\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
        for assessment, count in assessment_counts.items():
            print(f"  {assessment}: {count} ä¸ª")
        
    except Exception as e:
        print(f"\nâŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")


def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    try:
        classifier = OptimizedLegalClassifier()
        print("\nğŸ” äº¤äº’å¼æ³•å¾‹åŠŸèƒ½åˆ†ç±»å™¨")
        print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡ºç¨‹åº")
        print("ğŸ“ å¯ç”¨ç®¡è¾–åŒº: usa, california, utah, florida, texas, eu, germany, france, italy, spain, netherlands, reference")
        
        while True:
            print(f"\n{'-'*50}")
            try:
                feature_input = input("ğŸ¯ è¯·è¾“å…¥åŠŸèƒ½æè¿°: ").strip()
                if feature_input.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                if not feature_input:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„åŠŸèƒ½æè¿°")
                    continue
                
                # æ‰§è¡Œåˆ†ç±»
                result = classifier.classify_feature(feature_input)
                
                # æ˜¾ç¤ºç»“æœ
                print(f"\nğŸ“‹ åˆ†ç±»ç»“æœ:")
                print(f"  ğŸ·ï¸  {result.get('assessment', 'Unknown')}")
                print(f"  ğŸ“Š ç½®ä¿¡åº¦: {result.get('confidence', 'N/A')}")
                print(f"  ğŸ’­ æ¨ç†: {result.get('reasoning', 'N/A')}")
                
                if result.get('regulations'):
                    print(f"  âš–ï¸  ç›¸å…³æ³•è§„: {len(result['regulations'])} ä¸ª")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§!")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†å‡ºé”™: {e}")
                
    except Exception as e:
        print(f"âŒ æ— æ³•å¯åŠ¨äº¤äº’æ¨¡å¼: {e}")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "interactive":
        interactive_mode()
    else:
        main()